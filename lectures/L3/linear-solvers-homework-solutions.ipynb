{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACSE-3 (Numerical Methods) <a class=\"tocSkip\">\n",
    "\n",
    "## Lecture 3: Linear Solvers (Numerical Linear Algebra) <a class=\"tocSkip\">\n",
    "    \n",
    "### Homework Exercises - Solutions <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Homework\" data-toc-modified-id=\"Homework-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Homework</a></span><ul class=\"toc-item\"><li><span><a href=\"#Homework---Gaussian-elimination---3x3-example\" data-toc-modified-id=\"Homework---Gaussian-elimination---3x3-example-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Homework - Gaussian elimination - 3x3 example</a></span><ul class=\"toc-item\"><li><span><a href=\"#Solution---Gaussian-elimination---3x3-example\" data-toc-modified-id=\"Solution---Gaussian-elimination---3x3-example-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Solution - Gaussian elimination - 3x3 example</a></span></li></ul></li><li><span><a href=\"#Homework---LU-solve\" data-toc-modified-id=\"Homework---LU-solve-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Homework - LU solve</a></span><ul class=\"toc-item\"><li><span><a href=\"#Solution---LU-solve\" data-toc-modified-id=\"Solution---LU-solve-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Solution - LU solve</a></span></li></ul></li><li><span><a href=\"#Homework---Gauss-Jordan-elimination-(matrix-inversion)\" data-toc-modified-id=\"Homework---Gauss-Jordan-elimination-(matrix-inversion)-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Homework - Gauss-Jordan elimination (matrix inversion)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Solution---Gauss-Jordan-elimination-(matrix-inversion)\" data-toc-modified-id=\"Solution---Gauss-Jordan-elimination-(matrix-inversion)-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Solution - Gauss-Jordan elimination (matrix inversion)</a></span></li></ul></li><li><span><a href=\"#Homework---the-Thomas-algorithm-for-solving-tridiagonal-systems--[$\\star$]\" data-toc-modified-id=\"Homework---the-Thomas-algorithm-for-solving-tridiagonal-systems--[$\\star$]-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Homework - the Thomas algorithm for solving tridiagonal systems  [$\\star$]</a></span><ul class=\"toc-item\"><li><span><a href=\"#Solution---the-Thomas-algorithm\" data-toc-modified-id=\"Solution---the-Thomas-algorithm-1.4.1\"><span class=\"toc-item-num\">1.4.1&nbsp;&nbsp;</span>Solution - the Thomas algorithm</a></span></li></ul></li><li><span><a href=\"#Homework---round-off-error-example-(Gauss-Jordan-inversion)\" data-toc-modified-id=\"Homework---round-off-error-example-(Gauss-Jordan-inversion)-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Homework - round off error example (Gauss-Jordan inversion)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Solution---round-off-error-example-(Gauss-Jordan-inversion)\" data-toc-modified-id=\"Solution---round-off-error-example-(Gauss-Jordan-inversion)-1.5.1\"><span class=\"toc-item-num\">1.5.1&nbsp;&nbsp;</span>Solution - round off error example (Gauss-Jordan inversion)</a></span></li></ul></li><li><span><a href=\"#Homework---Iterative-method-3---the-Gauss-Seidel-method\" data-toc-modified-id=\"Homework---Iterative-method-3---the-Gauss-Seidel-method-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Homework - Iterative method 3 - the Gauss-Seidel method</a></span><ul class=\"toc-item\"><li><span><a href=\"#Solution---Iterative-method-3---the-Gauss-Seidel-method\" data-toc-modified-id=\"Solution---Iterative-method-3---the-Gauss-Seidel-method-1.6.1\"><span class=\"toc-item-num\">1.6.1&nbsp;&nbsp;</span>Solution - Iterative method 3 - the Gauss-Seidel method</a></span></li></ul></li><li><span><a href=\"#Homework---Iterative-method-4---SOR-(a)\" data-toc-modified-id=\"Homework---Iterative-method-4---SOR-(a)-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Homework - Iterative method 4 - SOR (a)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Solution---Iterative-method-4---SOR---(a)\" data-toc-modified-id=\"Solution---Iterative-method-4---SOR---(a)-1.7.1\"><span class=\"toc-item-num\">1.7.1&nbsp;&nbsp;</span>Solution - Iterative method 4 - SOR - (a)</a></span></li></ul></li><li><span><a href=\"#Homework---Iterative-method-4---SOR-(b)--[$\\star$]\" data-toc-modified-id=\"Homework---Iterative-method-4---SOR-(b)--[$\\star$]-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>Homework - Iterative method 4 - SOR (b)  [$\\star$]</a></span><ul class=\"toc-item\"><li><span><a href=\"#Solution---Iterative-method-4---SOR-(b)\" data-toc-modified-id=\"Solution---Iterative-method-4---SOR-(b)-1.8.1\"><span class=\"toc-item-num\">1.8.1&nbsp;&nbsp;</span>Solution - Iterative method 4 - SOR (b)</a></span></li></ul></li><li><span><a href=\"#Homework---Algorithm-comparison--[$\\star$]\" data-toc-modified-id=\"Homework---Algorithm-comparison--[$\\star$]-1.9\"><span class=\"toc-item-num\">1.9&nbsp;&nbsp;</span>Homework - Algorithm comparison  [$\\star$]</a></span><ul class=\"toc-item\"><li><span><a href=\"#Solution---Algorithm-comparison\" data-toc-modified-id=\"Solution---Algorithm-comparison-1.9.1\"><span class=\"toc-item-num\">1.9.1&nbsp;&nbsp;</span>Solution - Algorithm comparison</a></span></li></ul></li><li><span><a href=\"#Homework---Cramer's-rule-[$\\star\\star$]\" data-toc-modified-id=\"Homework---Cramer's-rule-[$\\star\\star$]-1.10\"><span class=\"toc-item-num\">1.10&nbsp;&nbsp;</span>Homework - Cramer's rule [$\\star\\star$]</a></span><ul class=\"toc-item\"><li><span><a href=\"#Solution---Cramer's-rule\" data-toc-modified-id=\"Solution---Cramer's-rule-1.10.1\"><span class=\"toc-item-num\">1.10.1&nbsp;&nbsp;</span>Solution - Cramer's rule</a></span></li></ul></li><li><span><a href=\"#Homework---Interpolation-(Vandermonde-&amp;-Newton)-matrices--[$\\star$]\" data-toc-modified-id=\"Homework---Interpolation-(Vandermonde-&amp;-Newton)-matrices--[$\\star$]-1.11\"><span class=\"toc-item-num\">1.11&nbsp;&nbsp;</span>Homework - Interpolation (Vandermonde &amp; Newton) matrices  [$\\star$]</a></span><ul class=\"toc-item\"><li><span><a href=\"#Solution---Interpolation-(Vandermonde-&amp;-Newton)-matrices\" data-toc-modified-id=\"Solution---Interpolation-(Vandermonde-&amp;-Newton)-matrices-1.11.1\"><span class=\"toc-item-num\">1.11.1&nbsp;&nbsp;</span>Solution - Interpolation (Vandermonde &amp; Newton) matrices</a></span></li></ul></li><li><span><a href=\"#Homework---Hilbert-matrix--[$\\star$]\" data-toc-modified-id=\"Homework---Hilbert-matrix--[$\\star$]-1.12\"><span class=\"toc-item-num\">1.12&nbsp;&nbsp;</span>Homework - Hilbert matrix  [$\\star$]</a></span><ul class=\"toc-item\"><li><span><a href=\"#Solution---Hilbert-matrix\" data-toc-modified-id=\"Solution---Hilbert-matrix-1.12.1\"><span class=\"toc-item-num\">1.12.1&nbsp;&nbsp;</span>Solution - Hilbert matrix</a></span></li></ul></li><li><span><a href=\"#Homework---Comparison-of-iterative-methods--[$\\star$]\" data-toc-modified-id=\"Homework---Comparison-of-iterative-methods--[$\\star$]-1.13\"><span class=\"toc-item-num\">1.13&nbsp;&nbsp;</span>Homework - Comparison of iterative methods  [$\\star$]</a></span><ul class=\"toc-item\"><li><span><a href=\"#Solution---Comparison-of-iterative-methods\" data-toc-modified-id=\"Solution---Comparison-of-iterative-methods-1.13.1\"><span class=\"toc-item-num\">1.13.1&nbsp;&nbsp;</span>Solution - Comparison of iterative methods</a></span></li></ul></li><li><span><a href=\"#Homework---Cubic-splines-(from-Lecture-1)--[$\\star\\star$]\" data-toc-modified-id=\"Homework---Cubic-splines-(from-Lecture-1)--[$\\star\\star$]-1.14\"><span class=\"toc-item-num\">1.14&nbsp;&nbsp;</span>Homework - Cubic splines (from Lecture 1)  [$\\star\\star$]</a></span><ul class=\"toc-item\"><li><span><a href=\"#Solution---Cubic-splines\" data-toc-modified-id=\"Solution---Cubic-splines-1.14.1\"><span class=\"toc-item-num\">1.14.1&nbsp;&nbsp;</span>Solution - Cubic splines</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%precision 16\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.linalg as sl\n",
    "import scipy.sparse as sp\n",
    "import scipy.sparse.linalg as spl\n",
    "\n",
    "# prints in a way that we can cut and paste directly into code\n",
    "from pprint import pprint\n",
    "\n",
    "# font sizes for plots\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = ['Arial', 'Dejavu Sans']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework - Gaussian elimination - 3x3 example \n",
    "\n",
    "Consider now the system of three linear equations for three unknowns:\n",
    "\n",
    "\\begin{align*}\n",
    "  2x + 3y - 4z &= 5, \\\\[5pt]\n",
    "  6x + 8y + 2z &= 3, \\\\[5pt]\n",
    "  4x + 8y - 6z &= 19.\n",
    "\\end{align*}\n",
    "\n",
    "To ensure you understand what our algorithm will need to do, write this in matrix form, form the corresponding augmented system and perform row operations until you get to upper-triangular form, find the solution using back substitution (**do this all with pen and paper**).\n",
    "\n",
    "Check your answer against a solution found using SciPy.\n",
    "\n",
    "You should find $x=-6$, $y=5$, $z=-1/2$.\n",
    "\n",
    "Check also that the upper triangular matrix you obtain in this process is consistent with that obtained by our LU decomposition code (without partial pivoting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution - Gaussian elimination - 3x3 example "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the steps you could take\n",
    "\n",
    "$$\n",
    "[A \\, | \\, \\boldsymbol{b}] = \n",
    "\\left[\n",
    "  \\begin{array}{rrr|r}\n",
    "    2 & 3 & -4 & 5 \\\\\n",
    "    6 & 8 & 2 & 3 \\\\\n",
    "    4 & 8 & -6 & 19 \n",
    "  \\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "Subtract 3 times the first row from the second and 2 times the first row from the third:\n",
    "\n",
    "$$\n",
    "\\left[\n",
    "  \\begin{array}{rrr|r}\n",
    "    2 & 3 & -4    & 5 \\\\\n",
    "    0 & -1 & 14 & -12 \\\\\n",
    "    0 & 2 &  2  & 9 \n",
    "  \\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "Now add 2 times the second row to the third:\n",
    "\n",
    "$$\n",
    "\\left[\n",
    "  \\begin{array}{rrr|r}\n",
    "    2 & 3  & -4    & 5 \\\\\n",
    "    0 & -1 & 14 & -12 \\\\\n",
    "    0 & 0  & 30  & -15 \n",
    "  \\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "So by back substitution we solve the final equation:\n",
    "\n",
    "$$30 z = -14 \\Rightarrow z = -\\frac{1}{2}$$\n",
    "\n",
    "The the second to last:\n",
    "\n",
    "$$ y = 12 + 14z = 12 -7 = 5$$\n",
    "\n",
    "and the third from last:\n",
    "\n",
    "$$ x = \\frac{1}{2}\\left( 5 + 4z - 3y \\right) = \\frac{1}{2}\\left( 5 -2 - 15 \\right) = -6 $$\n",
    "\n",
    "The upper-triangular matrix here should be the same as the $U$ obtained with our `LU decomposition` function (without partial pivoting) - let's check this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LU_decomposition(A):\n",
    "    # construct upper triangular matrix contains Gaussian elimination result\n",
    "    # we won't change A in-place but create a local copy\n",
    "    # if we don't do this then A will be over-written by the U we\n",
    "    # compute and return\n",
    "    A = A.copy()\n",
    "    m, n = A.shape\n",
    "    assert(m == n)\n",
    "    # For simplicity we set up a matrix to store L, but note the comment above\n",
    "    # that if we had memory concerns we could/should reuse zeroed entries in A.\n",
    "    # Note that we don't initialise this to the identity now, as this won't be \n",
    "    # correct when we use partial pivoting in a later updated versio.\n",
    "    L = np.zeros((n,n))\n",
    "    # Loop over each pivot row - don't need to consider the final row as a pivot\n",
    "    for k in range(n-1):\n",
    "        # Loop over each equation below the pivot row - now we do need to consider the last row\n",
    "        for i in range(k+1, n):\n",
    "            # Define the scaling factor outside the innermost\n",
    "            # loop otherwise its value gets changed.\n",
    "            s = (A[i, k] / A[k, k])\n",
    "            for j in range(k, n):\n",
    "                A[i, j] = A[i, j] - s*A[k, j]\n",
    "            # store the scaling factors which make up the lower tri matrix\n",
    "            L[i, k] = s\n",
    "    # remember to add in the ones on the main diagonal to L\n",
    "    L += np.eye(m)\n",
    "    # A now is the upper triangular matrix U, so return L and A(=U)\n",
    "    return L, A\n",
    "\n",
    "A = np.array([[2.,3.,-4.],[6.,8.,2.],[4.,8.,-6.]])\n",
    "b = np.array([5.,3.,19.])\n",
    "pprint(sl.inv(A) @ b)\n",
    "\n",
    "\n",
    "## and we can check our upper diagonal matrix we derived above against \n",
    "# that obtained by our LU decomposition function (without partial pivoting)\n",
    "\n",
    "L, U = LU_decomposition(A)\n",
    "print(U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework - LU solve\n",
    "\n",
    "Write and test a function to replicate \n",
    "[https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.lu_solve.html#scipy.linalg.lu_solve](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.lu_solve.html#scipy.linalg.lu_solve).\n",
    "\n",
    "You could try to formulate solutions which are based on either `LU_decomposition` or `LU_decomposition_pp`, i.e. with and without the use of partial pivoting.\n",
    "\n",
    "Test your solver against `scipy.linalg.lu_solve` using the problem from lecture:\n",
    "\n",
    "```Python \n",
    "A = np.array([[1., 0., 3., 7.], \n",
    "              [2., 1., 0., 4.],\n",
    "              [5., 4., 1., -2.], \n",
    "              [4., 1., 6., 2.]])\n",
    "b = np.array([1., 2., -3., 2.])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution - LU solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This version of the solution does not use partial pivoting \n",
    "# and hence doesn't need to deal with a P matrix\n",
    "\n",
    "def LU_decomposition(A):\n",
    "    # construct upper triangular matrix contains Gaussian elimination result\n",
    "    # we won't change A in-place but create a local copy\n",
    "    # if we don't do this then A will be over-written by the U we\n",
    "    # compute and return\n",
    "    A = A.copy()\n",
    "    m, n = A.shape\n",
    "    assert(m == n)\n",
    "    # For simplicity we set up a matrix to store L, but note the comment above\n",
    "    # that if we had memory concerns we would reuse zeroed entries in A.\n",
    "    # We don't initialise this to the identity now, as this won't be correct\n",
    "    # when we use partial pivoting a little later.\n",
    "    L = np.zeros((n,n))\n",
    "    # Loop over each pivot row - don't need to consider the final row as a pivot\n",
    "    for k in range(n-1):\n",
    "        # Loop over each equation below the pivot row - now we do need to consider the last row\n",
    "        for i in range(k+1, n):\n",
    "            # Define the scaling factor outside the innermost\n",
    "            # loop otherwise its value gets changed.\n",
    "            s = (A[i, k] / A[k, k])\n",
    "            for j in range(k, n):\n",
    "                A[i, j] = A[i, j] - s*A[k, j]\n",
    "            # store the scaling factors which make up the lower tri matrix\n",
    "            L[i, k] = s\n",
    "    # remember to add in the ones on the main diagonal to L\n",
    "    L += np.eye(m)\n",
    "    # A now is the upper triangular matrix U \n",
    "    return L, A\n",
    "\n",
    "# This function assumes that A is already an upper triangular matrix.\n",
    "def backward_substitution(A, b):\n",
    "    n = np.size(b)    \n",
    "    x = np.zeros(n)\n",
    "    for k in range(n-1, -1, -1):\n",
    "        s = 0.\n",
    "        for j in range(k+1, n):\n",
    "            s = s + A[k, j]*x[j]\n",
    "        x[k] = (b[k] - s)/A[k, k]    \n",
    "    return x\n",
    "\n",
    "# This function assumes that A is already a lower triangular matrix.\n",
    "def forward_substitution(A, b):\n",
    "    n = np.size(b)    \n",
    "    x = np.zeros(n)\n",
    "    for k in range(n):\n",
    "        s = 0.\n",
    "        for j in range(k):\n",
    "            s = s + A[k, j]*x[j]\n",
    "        x[k] = (b[k] - s)/A[k, k]\n",
    "    \n",
    "    return x\n",
    "\n",
    "def LU_solve(A, b):\n",
    "    \"\"\" An LU solve function that makes use of our\n",
    "    non partial pivoting versions of LU_decomposition\n",
    "    followed by forward_substitution and\n",
    "    backward_substitution\n",
    "    \"\"\" \n",
    "    L, U = LU_decomposition(A)\n",
    "    y = forward_substitution(L, b)\n",
    "    x = backward_substitution(U, y)\n",
    "    return x\n",
    "\n",
    "\n",
    "A = np.array([[1., 0., 3., 7.], \n",
    "              [2., 1., 0., 4.],\n",
    "              [5., 4., 1., -2.], \n",
    "              [4., 1., 6., 2.]])\n",
    "b = np.array([1., 2., -3., 2.])\n",
    "\n",
    "# Always check that our implementation produces the right result\n",
    "print(np.allclose(np.linalg.solve(A, b), LU_solve(A, b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This version of solution does use partial pivoting and so we do need to think about how to deal with P\n",
    "\n",
    "def swap_rows(A, j, k):\n",
    "    \"\"\" Swap rows j and k of matrix A\n",
    "    \"\"\"\n",
    "    # copy the two relevant rows\n",
    "    B = np.copy(A[j, :])\n",
    "    C = np.copy(A[k, :])\n",
    "    # and use them to update the relevant rows of A\n",
    "    A[k, :] = B\n",
    "    A[j, :] = C\n",
    "    return A\n",
    "\n",
    "def LU_decomposition_pp(A):\n",
    "    \"\"\" Perform LU decomposition on A with partial pivoting\n",
    "    Implementation does not overwrite A, uses a local copy\n",
    "    \"\"\"\n",
    "    m, n = A.shape\n",
    "    assert(m == n)\n",
    "    A = A.copy()  \n",
    "    # initialise L and P to the identity\n",
    "    L = np.zeros((m,m))\n",
    "    # call this P_ to emphasise that this is the permutation matrix on the LHS\n",
    "    # we will return the inverse of P (same as P transpose) to match SciPy behaviour\n",
    "    P_ = np.eye(m)\n",
    "    for k in range(m-1):\n",
    "        # Find the index of the largest ABSOLUTE value. np.argmax will return\n",
    "        # the index of the largest element in an array - different method to above\n",
    "        j = np.argmax(abs(A[k:, k]))\n",
    "        # j here will be the entry counting down from the k-th entry of A, \n",
    "        # so add on k for use as an index for the matrix A!\n",
    "        j += k   \n",
    "        # do the swap or A's rows\n",
    "        A = swap_rows(A, j, k)\n",
    "        # and record this swap in the matrix P_\n",
    "        P_ = swap_rows(P_, j, k)\n",
    "        L = swap_rows(L, j, k)\n",
    "        for i in range(k+1, m):\n",
    "            s = A[i, k]/A[k, k]\n",
    "# next line vectorises the row update from before \"for j in range(k, n):  A[i, j] = A[i, j] - s*A[k, j]\"\"\n",
    "            A[i, k:] -= A[k, k:]*s\n",
    "            L[i, k] = s\n",
    "    # the upper-triangular matrix will now be stored in A\n",
    "    U = A\n",
    "    # add in the diagonal of ones into L\n",
    "    L += np.eye(m)\n",
    "    # we need to return the transpose of P_ (same as inverse(P_)) to match SciPy output.\n",
    "    return P_.T, L, U\n",
    "\n",
    "\n",
    "# This function assumes that A is already an upper triangular matrix.\n",
    "def backward_substitution(A, b):\n",
    "    n = np.size(b)    \n",
    "    x = np.zeros(n)\n",
    "    for k in range(n-1, -1, -1):\n",
    "        s = 0.\n",
    "        for j in range(k+1, n):\n",
    "            s = s + A[k, j]*x[j]\n",
    "        x[k] = (b[k] - s)/A[k, k]    \n",
    "    return x\n",
    "\n",
    "# This function assumes that A is already a lower triangular matrix.\n",
    "def forward_substitution(A, b):\n",
    "    n = np.size(b)    \n",
    "    x = np.zeros(n)\n",
    "    for k in range(n):\n",
    "        s = 0.\n",
    "        for j in range(k):\n",
    "            s = s + A[k, j]*x[j]\n",
    "        x[k] = (b[k] - s)/A[k, k]\n",
    "    \n",
    "    return x\n",
    "\n",
    "def LU_solve(A, b):\n",
    "    \"\"\" An LU solve function that makes use of our\n",
    "    partial pivoting versions of LU_decomposition\n",
    "    followed by forward_substitution and\n",
    "    backward_substitution\n",
    "    \"\"\" \n",
    "    # A = P L U\n",
    "    P, L, U = LU_decomposition_pp(A)\n",
    "    # recall that P^{-1} = P^T - use this info to first update the RHS vector\n",
    "    Pinvb = P.T @ b\n",
    "    y = forward_substitution(L, Pinvb)\n",
    "    x = backward_substitution(U, y)\n",
    "    return x\n",
    "\n",
    "\n",
    "A = np.array([[1., 0., 3., 7.], \n",
    "              [2., 1., 0., 4.],\n",
    "              [5., 4., 1., -2.], \n",
    "              [4., 1., 6., 2.]])\n",
    "b = np.array([1., 2., -3., 2.])\n",
    "\n",
    "# Always check that our implementation produces the right result\n",
    "print(np.linalg.solve(A, b))\n",
    "print(LU_solve(A, b))\n",
    "print(np.allclose(np.linalg.solve(A, b), LU_solve(A, b)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework - Gauss-Jordan elimination (matrix inversion)\n",
    "\n",
    "The (Gaussian) elimination procedure we saw in the lecture is based on the fact that if $\\boldsymbol{b} = A\\boldsymbol{x}$, then row operations (multiplying by a nonzero scalar, and subtracting) on the matrix  $A$ have the same effect on the resulting matrix vector product $A\\boldsymbol{x}$ as when we apply the same operations directly on the vector $\\boldsymbol{b}$. \n",
    "\n",
    "Since we can write $\\boldsymbol{b}=I\\boldsymbol{b}$ where $I$ is the identity matrix, we can change the procedure by starting with the matrix $I$ on the right-hand side of the augmented matrix, and instead of applying the operations on $\\boldsymbol{b}$ directly we apply it to the identity matrix which step-by-step gets transformed into a different matrix $M$. \n",
    "\n",
    "I.e. consider the system \n",
    "\n",
    "$$A\\boldsymbol{x} = \\boldsymbol{b},$$\n",
    "\n",
    "if we apply the same row operations to $A$ as to $\\boldsymbol{b}$, up until $A$ has been transformed into the identity, then we have\n",
    "\n",
    "$$I\\boldsymbol{x} = \\hat{\\boldsymbol{b}},$$\n",
    "\n",
    "where the hat notation indicates that we have performed the same row operations on the matrix $\\boldsymbol{b}$. From which we simply have that $\\boldsymbol{x} = \\hat{\\boldsymbol{b}}$.\n",
    "\n",
    "Instead, suppose we start things off by writing \n",
    "\n",
    "$$A\\boldsymbol{x} = I\\boldsymbol{b},$$\n",
    "\n",
    "and again perform the row operations, but on the RHS now apply these to the matrix $I$ rather than the vector $\\boldsymbol{b}$.  \n",
    "\n",
    "If we continue the procedure until the matrix-part on the left of the augmented matrix has changed to the identity matrix (as in Gauss-Jordan elimination), we end up with a result $M\\boldsymbol{b}$ on the right-hand side for which\n",
    "\n",
    "$$I\\boldsymbol{x} = M{\\boldsymbol{b}},$$\n",
    "\n",
    "which we know should be equal to the solution of $A\\boldsymbol{x}=\\boldsymbol{b}$, in other words $M\\boldsymbol{b}=\\boldsymbol{x}=A^{-1}\\boldsymbol{b}$. \n",
    "\n",
    "But note that we have not used any information about $\\boldsymbol{b}$ within this process, i.e. $M\\boldsymbol{b}=A^{-1}\\boldsymbol{b}$ holds for arbitrary $\\boldsymbol{b}$ and we can therefore conclude that\n",
    "the matrix $M$ we obtain has to be equal to $A^{-1}$.\n",
    "\n",
    "We have thus derived an algorithm to actually invert the matrix $A$: \n",
    "\n",
    "our algorithm is to form the augmented equation with the full identity matrix in the place of the vector $\\boldsymbol{b}$, i.e. $[\\,A\\,|\\,I\\,]$, and perform row operations until $A$ is transformed into the identity matrix $I$, then we are left with the inverse of $A$ in the original $I$ location, i.e.\n",
    "\n",
    "$$ [\\,A\\,|\\,I\\,] \\longrightarrow [\\,I\\,|\\,A^{-1}\\,]. $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update our codes from the lecture to implement a method that constructs inverse matrix through row operations via the procedure described above.\n",
    "\n",
    "Check your results against the inverse matrix given by SciPy's inverse function using the matrix\n",
    "\n",
    "```Python\n",
    "A = np.array([[2., 3., -4.], [3., -1., 2.], [4., 2., 2.]])\n",
    "```\n",
    "\n",
    "Hint: Once you have performed Gaussian elimination to transform $A$ into an upper triangular matrix, you can perform another elimination \"from bottom to top\" to transform $A$ into a diagonal matrix - you will thus need to update `upper_triangle` so that it takes in a matrix and places this on the RHS of the augmented system, and also write a similar `lower_triangle` function to perform a similar elimination starting from the last row and eliminating all entries above the main diagonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution - Gauss-Jordan elimination (matrix inversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This updated version of the upper_triangular function now\n",
    "# assumes that a matrix, B, is in the old vector location (what was b)\n",
    "# in the augmented system, and applies the same operations to\n",
    "# B as to A - only a minor difference\n",
    "\n",
    "def upper_triangle2(A, B):\n",
    "    n, m = np.shape(A)\n",
    "    assert(n == m)  # this is designed to work for a square matrix\n",
    "\n",
    "    # Loop over each pivot row.\n",
    "    for k in range(n-1):\n",
    "        # Loop over each equation below the pivot row.\n",
    "        for i in range(k+1, n):\n",
    "            # Define the scaling factor outside the innermost\n",
    "            # loop otherwise its value gets changed as you are\n",
    "            # over-writing A\n",
    "            s = (A[i, k]/A[k, k])\n",
    "            for j in range(n):\n",
    "                A[i, j] = A[i, j] - s*A[k, j]\n",
    "                # replace the old b update with the same update as A\n",
    "                B[i, j] = B[i, j] - s*B[k, j]\n",
    "\n",
    "\n",
    "# and this is a version which transforms the matrix into lower\n",
    "# triangular form - the point here is that if you give it a\n",
    "# matrix that is already in upper triangular form, then the\n",
    "# result will be a diagonal matrix\n",
    "def lower_triangle2(A, B):\n",
    "    n, m = np.shape(A)\n",
    "    assert(n == m)  # this is designed to work for a square matrix\n",
    "\n",
    "    # now it's basically just the upper triangular algorithm \n",
    "    # applied backwards\n",
    "    for k in range(n-1, -1, -1):\n",
    "        for i in range(k-1, -1, -1):\n",
    "            s = (A[i, k]/A[k, k])\n",
    "            for j in range(n):\n",
    "                A[i, j] = A[i, j] - s*A[k, j]\n",
    "                B[i, j] = B[i, j] - s*B[k, j]\n",
    "\n",
    "\n",
    "# Let's redefine A as our matrix above\n",
    "A = np.array([[2., 3., -4.], [3., -1., 2.], [4., 2., 2.]])\n",
    "\n",
    "# and B is the identity of the corresponding size\n",
    "B = np.eye(np.shape(A)[0])\n",
    "\n",
    "# transform A into upper triangular form \n",
    "# (and perform the same operations on B)\n",
    "upper_triangle2(A, B)\n",
    "print('Upper triangular transformed A = ')\n",
    "pprint(A)\n",
    "\n",
    "# now make this updated A lower triangular as well \n",
    "# (the result should be diagonal)\n",
    "lower_triangle2(A, B)\n",
    "print('\\nand following application of our lower triangular function = ')\n",
    "pprint(A)\n",
    "\n",
    "# The final step to achieve the identity is just to divide each row through by the value \n",
    "# of the diagonal to end up with 1's on the main diagonal and 0 everywhere else.\n",
    "for i in range(np.shape(A)[0]):\n",
    "    B[i, :] = B[i, :]/A[i, i]\n",
    "    A[i, :] = A[i, :]/A[i, i]\n",
    "\n",
    "# the final A should be the identity\n",
    "print('\\nOur final transformed A = ')\n",
    "pprint(A)\n",
    "\n",
    "# the final B should therefore be the inverse of the original B\n",
    "print('\\nand the correspondingly transformed B = ')\n",
    "pprint(B)\n",
    "\n",
    "# let's compute the inverse using built-in functions and check\n",
    "# we get the same answer (we need to reinitialise A)\n",
    "A = np.array([[2., 3., -4.], [3., -1., 2.], [4., 2., 2.]])\n",
    "print('\\nSciPy computes the inverse as:')\n",
    "pprint(sl.inv(A))\n",
    "\n",
    "# B should now store the inverse of the original A - let's check\n",
    "print('\\nSuccess: ', np.allclose(B, sl.inv(A)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework - the Thomas algorithm for solving tridiagonal systems  [$\\star$]\n",
    "\n",
    "As we will see in later lectures when solving PDEs (and BVPs - basically something with a spatial derivative) it is common to find ourselves having to solve [*tridiagonal systems*](https://en.wikipedia.org/wiki/Tridiagonal_matrix).\n",
    "\n",
    "The Thomas algorithm can be considered as a simplified/specialised case of Gaussian elimination for this type of problem which has complexity $\\mathcal{O}(n)$ rather than the $\\mathcal{O}(n^3)$ required for general Gaussian elimination.\n",
    "\n",
    "A description of the algorithm is given here \n",
    "<a href=\"https://www.cfd-online.com/Wiki/Tridiagonal_matrix_algorithm_-_TDMA_(Thomas_algorithm)\">https://www.cfd-online.com/Wiki/Tridiagonal_matrix_algorithm_-_TDMA_(Thomas_algorithm)</a> \n",
    "and here\n",
    "[https://en.wikipedia.org/wiki/Tridiagonal_matrix_algorithm](https://en.wikipedia.org/wiki/Tridiagonal_matrix_algorithm).\n",
    "\n",
    "[My sample solution will use the notation and algorithm as presented at the first of these links.]\n",
    "\n",
    "Implement the algorithm and apply it to the tridiagonal matrix system given by \n",
    "\n",
    "```Python\n",
    "n = 50\n",
    "main_diag = np.ones(n)  \n",
    "off_diag = np.random.random(n-1)\n",
    "A = np.diag(-2*main_diag, 0) + np.diag(1.*off_diag, 1) + np.diag(1.*off_diag, -1)\n",
    "b = np.random.random(A.shape[0])\n",
    "```\n",
    "\n",
    "you can compare your solution against that obtained with SciPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution - the Thomas algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thomas_solver(a, b, c, d):\n",
    "    \"\"\" Thomas algorithm for solution of a tridiagonal matrix.\n",
    "    \n",
    "    a, b, c are the three vector diagonals of the TDMA solver, \n",
    "    in the order lower, main, upper\n",
    "    d is the RHS vector.\n",
    "    \n",
    "    Note this implementation takes copies of the vectors so as not\n",
    "    to overwrites them.\n",
    "    \n",
    "    this implementation uses the notation and psedo-code here\n",
    "    https://www.cfd-online.com/Wiki/Tridiagonal_matrix_algorithm_-_TDMA_(Thomas_algorithm)\n",
    "    \"\"\"\n",
    "    n = len(d) \n",
    "    assert (len(b) == n)\n",
    "    assert (len(a) == n-1)\n",
    "    assert (len(c) == n-1)\n",
    "    ac = np.copy(a)\n",
    "    bc = np.copy(b)\n",
    "    cc = np.copy(c)\n",
    "    dc = np.copy(d)\n",
    "    x = np.zeros_like(d)\n",
    "    for k in range(1, n):\n",
    "        # this implementation uses the notation and psedo-code here\n",
    "        # https://www.cfd-online.com/Wiki/Tridiagonal_matrix_algorithm_-_TDMA_(Thomas_algorithm)\n",
    "        # be careful about indexing as our vectors all start at index zero\n",
    "        # which differs from pseudo-code assumptions\n",
    "        m = ac[k-1]/bc[k-1]\n",
    "        bc[k] = bc[k] - m * cc[k-1] \n",
    "        dc[k] = dc[k] - m * dc[k-1]\n",
    "    x[-1] = dc[-1] / bc[-1]\n",
    "    for k in range(n-2, -1, -1):\n",
    "        x[k] = (dc[k] - cc[k] * x[k+1]) / bc[k]\n",
    "    return x\n",
    "\n",
    "n = 50\n",
    "main_diag = np.ones(n)  \n",
    "off_diag = np.random.random(n-1)\n",
    "A = np.diag(-2*main_diag, 0) + np.diag(1.*off_diag, 1) + np.diag(1.*off_diag, -1)\n",
    "b = np.random.random(A.shape[0])\n",
    "\n",
    "# now extract the diagonals from matrix\n",
    "aa = np.diag(A, k=-1)\n",
    "bb = np.diag(A, k=0)\n",
    "cc = np.diag(A, k=1)\n",
    "\n",
    "x = thomas_solver(aa,bb,cc,b)\n",
    "print(np.allclose(x, sl.solve(A,b)))\n",
    "print(sl.norm(A@x - b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework - round off error example (Gauss-Jordan inversion)\n",
    "\n",
    "In the lecture we saw an issue with round off error with the LU algorithm and the matrix \n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "\\epsilon & 1 \\\\\n",
    "1 & 1 \\\\\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Write a function `def Gauss_Jordan_inversion(A):` which inverts a matrix $A$ using appropriate calls to back and then forward substitution.\n",
    "\n",
    "Compare your result for the inverse matrix with that obtained from SciPy for increasingly small values of $\\epsilon$, e.g. down to $10^{-16}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution - round off error example (Gauss-Jordan inversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to perform Gauss-Jordan inversion based on the code we used above\n",
    "def Gauss_Jordan_inversion(A):\n",
    "    # B is the identity of the corresponding size\n",
    "    B = np.eye(np.shape(A)[0])\n",
    "    # transform A into upper triangular form (and perform the same operations on B)\n",
    "    upper_triangle2(A, B)\n",
    "    # now make this updated A lower triangular as well (the result should be diagonal)\n",
    "    lower_triangle2(A, B)\n",
    "    # final step is just to divide each row through by the value \n",
    "    # of the diagonal to end up with the identity in the place of A\n",
    "    for i in range(np.shape(A)[0]):\n",
    "        B[i, :] = B[i, :]/A[i, i]\n",
    "        A[i, :] = A[i, :]/A[i, i]\n",
    "    return A, B\n",
    "\n",
    "# call the function for our matrix A with a series of increasinly small epsilon values\n",
    "for epsilon in [1.e-6, 1.e-8, 1.e-10, 1.e-12, 1.e-14, 1.e-15, 1.e-16]:\n",
    "    A = np.array([[epsilon, 1],[1,1]])\n",
    "    A, B = Gauss_Jordan_inversion(A)\n",
    "    # let's compute the inverse using built-in functions and check we get the same answer\n",
    "    Ainv = sl.inv(np.array([[epsilon, 1],[1,1]]))\n",
    "    print('\\nFor epsilon = {0:.1e}, B[0,0] - sl.inv(A)[0,0] = {1:.8e} '.format(epsilon, B[0,0] - Ainv[0,0]))\n",
    "    print('np.allclose(B,Ainv): ',np.allclose(B,Ainv))\n",
    "\n",
    "print('\\nIn the final epsilon case we have the matrices')\n",
    "print('\\nsl.inv(np.array([[epsilon, 1],[1,1]])) = ')\n",
    "pprint(Ainv)\n",
    "print('\\nOur Gauss-Jordan based inverse = ')\n",
    "pprint(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework - Iterative method 3 - the Gauss-Seidel method\n",
    "\n",
    "\n",
    "We can make a small improvement to Jacobi's method using the updated components of the solution vector as soon as they become available:\n",
    "\n",
    "* Starting from a guess at the solution $\\boldsymbol{x}^{(0)}$\n",
    "\n",
    "\n",
    "* iterate for $k>0$\n",
    "$$x_i^{(k)} = \\frac{1}{a_{ii}}\\left(b_i- \\sum_{\\substack{j=1\\\\ j< i}}^n a_{ij}x_j^{(k)} - \\sum_{\\substack{j=1\\\\ j> i}}^n a_{ij}x_j^{(k-1)}\\right),\\quad  i=1,2,\\ldots, n.$$\n",
    "\n",
    "\n",
    "Note that as opposed to Jacobi, we can overwrite the entries of $\\boldsymbol{x}$ as they are updated; with Jacobi we need to store both the new as well as the old iteration (i.e. not overwrite the old entries until we have finished with them - which was not until the end of every iteration).\n",
    "\n",
    "As we are using updated knowledge immediately, the Gauss-Seidel algorithm should converge faster than Jacobi, but note that this convergence can only be *guaranteed* for matrices which are diagonally dominant (for every row, the magnitude of value on the main diagonal is greater than the sum of the magnitudes of all the other entries in that row), or if the matrix is *symmetric positive definite*.  \n",
    "\n",
    "Generalise the Jacobi code to solve the matrix problem using Gauss-Seidel's method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution - Iterative method 3 - the Gauss-Seidel method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss_seidel(A, b, maxit=500):\n",
    "    m, n = A.shape\n",
    "    x = np.zeros_like(b)\n",
    "    for k in range(maxit):\n",
    "        for i in range(m):\n",
    "            # note that here we will be re-using the x as we update it each i\n",
    "            x[i] = 1/A[i, i] * ( b[i] - ( A[i, :i] @ x[:i] ) - ( A[i, i+1:] @ x[i+1:] ) )\n",
    "    return x\n",
    "\n",
    "\n",
    "A = np.array([[10., 2., 3., 5.],\n",
    "              [1., 14., 6., 2.],\n",
    "              [-1., 4., 16., -4],\n",
    "              [5., 4., 3., 11.]])\n",
    "\n",
    "b = np.array([1., 2., 3., 4.])\n",
    "\n",
    "np.allclose( ( sl.inv(A) @ b ), gauss_seidel(A, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework - Iterative method 4 - SOR (a)\n",
    "\n",
    "Convergence of the Gauss-Seidel method can be improved by a technique known as successive over relaxation (SOR). \n",
    "\n",
    "The idea is to take the new value of $x_i$ as a weighted average of its previous value and the value predicted by the regular Gauss-Seidel iteration. \n",
    "\n",
    "The corresponding formula for the $k^{th}$ iteration of the algorithm and the $i^{th}$ row is:\n",
    "\n",
    "$$x_i^{(k)} = \\frac{\\omega}{a_{ii}}\\left(b_i- \\sum_{\\substack{j=1}}^{i-1} a_{ij}x_j^{(k)} - \\sum_{\\substack{j=i+1}}^n a_{ij}x_j^{(k-1)}\\right) + (1-\\omega)x_i^{(k-1)},\\;\\;\\;\\;  i=1,2,\\ldots, n.$$\n",
    "\n",
    "where the weight $\\omega$ is called the *relaxation factor* and is usually positive. Note that when $\\omega > 1$, the method is called *over-relaxation*, whereas values  $0 < \\omega < 1$ lead to under-relaxation. This section and the method has the title S**O**R since it is the careful choice of $\\omega > 1$ that accelerates convergence.\n",
    "\n",
    "\n",
    "- What does the algorithm give for $\\omega = 0$, for $\\omega = 1$, for $0 < \\omega < 1$ ? \n",
    "\n",
    "\n",
    "- Write a function that solves a system with the relaxed Gauss-Seidel's algorithm, for a given $\\omega$.\n",
    "\n",
    "\n",
    "- Use this function to solve the system from Lecture 7,  for different values of $\\omega$. How many iterations are necessary to reach a tolerance of $10^{-6}$ for each value of $\\omega$ ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution - Iterative method 4 - SOR - (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[10., 2., 3., 5.],\n",
    "              [1., 14., 6., 2.],\n",
    "              [-1., 4., 16., -4],\n",
    "              [5., 4., 3., 11.]])\n",
    "b = np.array([1., 2., 3., 4.])\n",
    "\n",
    "\n",
    "def gauss_seidel_rel_V1(A, b, omega, maxit=500, tol=1.e-6):\n",
    "    m, n = A.shape\n",
    "    x = np.zeros_like(b)\n",
    "    for k in range(maxit):\n",
    "        for i in range(m):\n",
    "            x[i] = omega/A[i, i] * ( b[i] - ( A[i, :i] @ x[:i] ) - ( A[i, i+1:] @ x[i+1:] ) ) + (1. - omega)*x[i]\n",
    "        residual = np.linalg.norm(( A @ x ) - b)\n",
    "#        print(\"iteration: %d    residual: %e\" %(k,residual))\n",
    "        if (residual < tol):\n",
    "            break\n",
    "\n",
    "    return x, k\n",
    "\n",
    "# for omega = 0: nothing happens and the system is not solved\n",
    "# for omega = 1: standard Gauss-Seidel\n",
    "# for 0 < omega < 1: the new x is taken as an average of the old x and the new one.\n",
    "# Instead of moving directly from x^(k) to the x^(k+1) given by GS, you stop somewhere in the middle\n",
    "#  ==> usually slows-down the convergence\n",
    "\n",
    "\n",
    "for omega in [0.5, 0.9, 0.95, 1, 1.1, 1.5]:\n",
    "    x, i = gauss_seidel_rel_V1(A, b, omega)\n",
    "    print(\"For omega = %1.2f,   %d relaxed GS iterations are needed to reach a tolerance of 1e-6\" % (omega, i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework - Iterative method 4 - SOR (b)  [$\\star$]\n",
    "\n",
    "$\\omega$ cannot be determined beforehand for an arbitrary system, \n",
    "however, an estimate can be computed during run time. \n",
    "\n",
    "Let $\\Delta_x^{(k)} = | x^{(k)} - x^{(k-1)} |$ be the magnitude of the change in x during the $k^{th}$ iteration. \n",
    "If $k$ is sufficiently large (say $k \\geq 5$), it can be shown that an approximation of the optimal value of $\\omega$ is:\n",
    "$$\n",
    "\\omega_{opt} \\approx \\frac{2}{1+\\sqrt{1-\\Delta x^{(k+1)} / \\Delta x^{(k)}}} \\,.\n",
    "$$\n",
    "\n",
    "The relaxed Gauss-Seidel algorithm can be summarised as follows:  \n",
    "\n",
    "- Carry out $k$ iterations with $\\omega = 1$ (usually $k=10$ for big systems)  \n",
    "\n",
    "\n",
    "- Record \t$\\Delta x^{(k)}$  \n",
    "\n",
    "\n",
    "- Perform an additional iteration  \n",
    "\n",
    "\n",
    "- Record \t$\\Delta x^{(k+1)}$  \n",
    "\n",
    "\n",
    "- Compute $\\omega_{opt}$  \n",
    "\n",
    "\n",
    "- Perform all subsequent iterations with $\\omega = \\omega_{opt}$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Modify previous function to compute automatically the relaxation parameter $\\omega$. Compute $\\omega_{opt}$ after $k=6$ iterations as the system is small.\n",
    " \n",
    "Solve the previous system with this new function. What is the value of $\\omega$ ? How many iterations are necessary to reach a tolerance of $10^{-6}$ ?\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution - Iterative method 4 - SOR (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "def gauss_seidel_rel_V2(A, b, k_relax=10, omega_ini=1., maxit=500, tol=1.e-6):\n",
    "    m, n = A.shape\n",
    "    x = np.zeros_like(b)\n",
    "    omega = omega_ini\n",
    "    for k in range(maxit):\n",
    "        for i in range(m):\n",
    "            x[i] = omega/A[i,i] * ( b[i] - ( A[i, :i] @ x[:i] ) - ( A[i, i+1:] @ x[i+1:] ) ) + (1. - omega)*x[i]\n",
    "        residual = np.linalg.norm(( A @ x ) - b)\n",
    "#        print(\"iteration: %d    residual: %e\" %(k,residual))\n",
    "        if (residual < tol): break\n",
    "        \n",
    "        if (k == k_relax): \n",
    "            res_prev = residual\n",
    "        if (k == k_relax+1): \n",
    "            res = residual\n",
    "            omega = 2/(1+sqrt(1-res/res_prev))\n",
    "    return x,k,omega\n",
    "\n",
    "k = 6\n",
    "x,i,omega = gauss_seidel_rel_V2(A, b, k_relax=k)\n",
    "print(\"Omega given by the formula after %d iterations: %1.3f.  %d relaxed GS iterations are needed to reach a tolerance of 1e-6\" % (k,omega,i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework - Algorithm comparison  [$\\star$]\n",
    "\n",
    "Let's consider $A\\boldsymbol{x}=\\boldsymbol{b}$ where:\n",
    "\n",
    "$$\n",
    "A = \\begin{pmatrix}\n",
    "5 & -2 & 0 & 0 & \\cdots & 0 \\\\\n",
    "-2 & 5 & -2 & 0 & \\cdots & 0 \\\\\n",
    "0 & -2 & 5 & -2 & \\cdots & 0 \\\\\n",
    "\\vdots & & & \\ddots & & \\vdots \\\\ \n",
    " & & & & 5 & -2 \\\\\n",
    "0 & \\cdots & & & -2 & 5 \\\\ \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "and\n",
    "$$\n",
    "b = \\left(0, 0, \\ldots, 0, 1000  \\right)^T\n",
    "$$\n",
    "\n",
    " - Solve $A\\boldsymbol{x}=\\boldsymbol{b}$ using the relaxed Gauss-Seidel algorithm for $n=3000$. Compare the number of iterations with the algorithm without relaxation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution - Algorithm comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3000\n",
    "A_big = 5*np.eye(n)\n",
    "for i in range(n-1):\n",
    "    A_big[i,i+1] = -2.\n",
    "    A_big[i+1,i] = -2.\n",
    "b_big = np.zeros(n)\n",
    "b_big[n-1] = 1000.\n",
    "\n",
    "x,i,omega = gauss_seidel_rel_V2(A_big, b_big, k_relax=-2)\n",
    "print(\"Without relaxation (omega=%1.3f) %d iterations are required to reach a tolerance of 1e-6\" % (omega,i))\n",
    "\n",
    "x,i,omega = gauss_seidel_rel_V2(A_big, b_big, k_relax=10)\n",
    "print(\"With relaxation (omega=%1.3f) %d iterations are required to reach a tolerance of 1e-6\" % (omega,i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework - Cramer's rule [$\\star\\star$]\n",
    "\n",
    "Write a recursive function to calculate the determinant by expanding out the first row of a matrix, using the formulae here for example: <https://en.wikipedia.org/wiki/Determinant>\n",
    "\n",
    "Hint: you may find  `np.r_` or `np.concatenate` useful to construct the indices required to form a minor of the matrix - a key line of my solution for example reads\n",
    "               \n",
    "```python\n",
    "def determinant(A):\n",
    ".\n",
    ".\n",
    ".\n",
    "    det += (-1.)**col * A[0, col] * determinant( A[1:, np.r_[0:col,col+1:n]] )\n",
    "```\n",
    "\n",
    "Use this determinant function to implement Cramer's rule: [https://en.wikipedia.org/wiki/Cramer%27s_rule](https://en.wikipedia.org/wiki/Cramer%27s_rule)\n",
    "\n",
    "and test on the system from lectures:\n",
    "\n",
    "```python\n",
    "A = np.array([[2.,3.,-4.], [6.,8.,2.], [4.,8.,-6.]])\n",
    "b = np.array([5.,3.,19.])\n",
    "```\n",
    "\n",
    "You can also compare timings for the different solvers we have encountered (our `gaussian_elimination` function, as well as SciPy's use of `inv` and `solve`), e.g. I considered the matrix\n",
    "\n",
    "```Python\n",
    "A_big = 5*np.eye(n)\n",
    "for i in range(n-1):\n",
    "    A_big[i,i+1] = -2.\n",
    "    A_big[i+1,i] = -2.\n",
    "b_big = np.zeros(n)\n",
    "b_big[n-1] = 1000.\n",
    "```\n",
    "and used something like this to compute a timing\n",
    "```Python\n",
    "tc = %timeit -n 10 -r 3 -o xc=cramers_rule(A_big,b_big)\n",
    "```\n",
    "Check the `timeit` docs to see what this does. You can extract the best time using `tc.best`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution - Cramer's rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determinant(A):\n",
    "    n = A.shape[0]\n",
    "    # check matrix is square\n",
    "    assert(n == A.shape[1])\n",
    "    if n == 1: \n",
    "        return A[0,0]\n",
    "    elif n==2: \n",
    "        return A[0,0] * A[1,1] - A[0,1] * A[1,0]\n",
    "    else:\n",
    "        det = 0.0\n",
    "        for col in range(n):\n",
    "            # expand out the first row: \n",
    "            # -1 to the power of entry number, multiplied by entry, multiplied by det of minor matrix\n",
    "            det += (-1.)**col * A[0, col] * determinant( A[1:, np.r_[0:col,col+1:n]] )\n",
    "    return det\n",
    "\n",
    "A = np.array([[10., 2., 1.], [6., 5., 4.], [1., 4., 7.]])\n",
    "print(determinant(A))\n",
    "print(np.isclose(determinant(A),sl.det(A)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cramers_rule(A,b):\n",
    "    n = np.size(b)\n",
    "    # check matrix is square\n",
    "    assert(A.shape[0] == A.shape[1])    \n",
    "    # and same number of columns as b\n",
    "    assert(A.shape[0] == n)\n",
    "    # compute and store determinant of A as we will use multiple times\n",
    "    detA = determinant(A)\n",
    "    # check it's non-zero, i.e. A is non-singular\n",
    "    assert(detA != 0)\n",
    "    x = np.zeros_like(b)\n",
    "    # loop over columns\n",
    "    for col in range(n):\n",
    "        # take a copy of A\n",
    "        Ab = np.copy(A)\n",
    "        # replace column col with b\n",
    "        Ab[:,col] = b\n",
    "        # i-th component of solution vecotr\n",
    "        x[col] = determinant(Ab)/detA\n",
    "    return x\n",
    "\n",
    "# check with our 3x3 example from lecture\n",
    "A = np.array([[2.,3.,-4.], [6.,8.,2.], [4.,8.,-6.]])\n",
    "b = np.array([5.,3.,19.])\n",
    "pprint(sl.inv(A) @ b)\n",
    "pprint(cramers_rule(A,b))\n",
    "\n",
    "time_scipy = %timeit -n 10 -r 3 -o x=sl.inv(A) @ b\n",
    "time_cramer = %timeit -n 10 -r 3 -o x=cramers_rule(A,b)\n",
    "           \n",
    "print(time_scipy.best, time_cramer.best, time_cramer.best/time_scipy.best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upper_triangle(A, b):\n",
    "    \"\"\" A function to covert A into upper triangluar form through row operations.\n",
    "    The same row operations are performed on the vector b.\n",
    "    \n",
    "    Note that this implementation does not use partial pivoting which is introduced below.\n",
    "    \n",
    "    Also note that A and b are overwritten, and hence we do not need to return anything\n",
    "    from the function.\n",
    "    \"\"\"\n",
    "    n = np.size(b)\n",
    "    rows, cols = np.shape(A)\n",
    "    # check A is square\n",
    "    assert(rows == cols)\n",
    "    # and check A has the same numner of rows as the size of the vector b\n",
    "    assert(rows == n)\n",
    "\n",
    "    # Loop over each pivot row - all but the last row which we will never need to use as a pivot\n",
    "    for k in range(n-1):\n",
    "        # Loop over each row below the pivot row, including the last row which we do need to update\n",
    "        for i in range(k+1, n):\n",
    "            # Define the scaling factor for this row outside the innermost loop otherwise \n",
    "            # its value gets changed as you over-write A!!\n",
    "            # There's also a performance saving from not recomputing things when not strictly necessary\n",
    "            s = (A[i, k] / A[k, k])\n",
    "            # Update the current row of A by looping over the column j\n",
    "            # start the loop from k as we can assume the entries before this are already zero\n",
    "            for j in range(k, n):\n",
    "                A[i, j] = A[i, j] - s*A[k, j]\n",
    "            # and update the corresponding entry of b\n",
    "            b[i] = b[i] - s*b[k]\n",
    "\n",
    "def back_substitution(A, b):\n",
    "    \"\"\" Function to perform back subsitution on the system Ax=b.\n",
    "    \n",
    "    Returns the solution x.\n",
    "    \n",
    "    Assumes that A is on upper triangluar form.\n",
    "    \"\"\"\n",
    "    n = np.size(b)\n",
    "    # Check A is square and its number of rows and columns same as size of the vector b\n",
    "    rows, cols = np.shape(A)\n",
    "    assert(rows == cols)\n",
    "    assert(rows == n)\n",
    "    # We can/should check that A is upper triangular using np.triu which is the \n",
    "    # upper triangular part of a matrix - if A is already upper triangular, then\n",
    "    # it should of course match the upper-triangular component of A!!\n",
    "    assert(np.allclose(A, np.triu(A)))\n",
    "    \n",
    "    x = np.zeros(n)\n",
    "    # start at the end (row n-1) and work backwards\n",
    "    for k in range(n-1, -1, -1):\n",
    "        # note that we could do this update in a single vectorised line \n",
    "        # using np.dot or @ - this could also speed things up\n",
    "        s = 0.\n",
    "        for j in range(k+1, n):\n",
    "            s = s + A[k, j]*x[j]\n",
    "        x[k] = (b[k] - s)/A[k, k]\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function for gaussian elimination (no pp) based on calls\n",
    "# to upper_triangle and then back_substitution.\n",
    "def gaussian_elimination(A,b):\n",
    "    Acopy = A.copy()\n",
    "    bcopy = b.copy()\n",
    "    upper_triangle(Acopy,bcopy)\n",
    "    x = back_substitution(Acopy, bcopy)\n",
    "    return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now compare the timings of methods using different matrix sizes\n",
    "# note no use of sparse data stucutres here\n",
    "\n",
    "ns = np.array([3, 4, 5, 6, 10, 20, 40, 80, 160, 320, 640])\n",
    "# an upper limit on problem sizes to apply Cramer to!\n",
    "ns_max_cramer = 8\n",
    "# an upper limit on problem sizes to apply GE to!\n",
    "ns_max_ge = 100\n",
    "time_scipy_inv = np.zeros(len(ns))\n",
    "time_scipy_sol = np.zeros(len(ns))\n",
    "time_cramer = np.zeros(len(ns))\n",
    "time_ge = np.zeros(len(ns))\n",
    "for it, n in enumerate(ns):\n",
    "    A_big = 5*np.eye(n)\n",
    "    for i in range(n-1):\n",
    "        A_big[i,i+1] = -2.\n",
    "        A_big[i+1,i] = -2.\n",
    "    b_big = np.zeros(n)\n",
    "    b_big[n-1] = 1000.\n",
    "    \n",
    "    tsi = %timeit -n 10 -r 3 -o xs=sl.inv(A_big) @ b_big\n",
    "    time_scipy_inv[it] = tsi.best\n",
    "    tss = %timeit -n 10 -r 3 -o xs=sl.solve(A_big, b_big)\n",
    "    time_scipy_sol[it] = tss.best\n",
    "    if n < ns_max_ge:\n",
    "        tg = %timeit -n 10 -r 3 -o xc=gaussian_elimination(A_big,b_big)\n",
    "        time_ge[it] = tg.best    \n",
    "        # might as well check our results agree with Scipy\n",
    "        print(np.allclose(sl.inv(A_big) @ b_big, gaussian_elimination(A_big,b_big)))\n",
    "    if n < ns_max_cramer:\n",
    "        tc = %timeit -n 10 -r 3 -o xc=cramers_rule(A_big,b_big)\n",
    "        time_cramer[it] = tc.best\n",
    "        # might as well check our results agree with Scipy\n",
    "        print(np.allclose(sl.inv(A_big) @ b_big, cramers_rule(A_big,b_big)))\n",
    "    \n",
    "# set up figure\n",
    "fig = plt.figure(figsize=(7, 7))\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "ax1.loglog(ns, time_scipy_inv, 'ko-', label='SciPy inverse')\n",
    "ax1.loglog(ns, time_scipy_sol, 'bo-', label='SciPy solve')\n",
    "ax1.loglog(ns[np.where(ns<ns_max_ge)[0]], time_ge[np.where(ns<ns_max_ge)[0]], 'go-', label='Gaussian elim')\n",
    "\n",
    "ax1.loglog(ns[np.where(ns<ns_max_cramer)[0]], time_cramer[np.where(ns<ns_max_cramer)[0]], 'ro-', label=\"Cramer's rule\")\n",
    "ax1.set_xlabel('$n$', fontsize=16)\n",
    "ax1.set_ylabel('%Timeit best time [seconds]', fontsize=16)\n",
    "ax1.set_title('Linear solve cost', fontsize=16)\n",
    "ax1.legend(loc='best', fontsize=14)\n",
    "ax1.grid(True)\n",
    "\n",
    "\n",
    "# save figure to use in lecture\n",
    "#fig.savefig('solver_timing_plot.png', dpi=600, format='png', facecolor='w', edgecolor='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework - Interpolation (Vandermonde & Newton) matrices  [$\\star$]\n",
    "\n",
    "We recalled at the start of this lecture how polynomial interpolation (Lecture 1) using a monomial basis involved solution of a linear system with a Vandermonde matrix.\n",
    "\n",
    "A potential issue is that Vandermonde matrices are known to be ill-conditioned (we saw this in ACSE-2).\n",
    "\n",
    "Write some code to construct the Vandermonde matrix and investigate its condition numbers for a uniform mesh of data points in $x$ (i.e. `x = np.linspace(0, 1, n)` as a function of the number of points (`n`).\n",
    "\n",
    "\n",
    "In lecture 1 we also considered alternative basis functions and went through a derivation of the Newton polynomial (we noted that it was easier to implement than the Lagrange polynomial) - we should be able to recognise now that our derivation was essentially conducting back substitution based on a particular lower-triangular matrix!\n",
    "\n",
    "For details see:\n",
    "https://en.wikipedia.org/wiki/Newton_polynomial#Main_idea\n",
    "\n",
    "Write some code to construct the 'Newton matrix' described at this web link, and as for the Vandermonde matrix investigate its condition number for a uniform mesh of data points in $x$ as a function of the number of points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution - Interpolation (Vandermonde & Newton) matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the Vandermonde matrix\n",
    "\n",
    "def vdm_row(alpha, n):\n",
    "    row = np.zeros(n)\n",
    "    cur_alpha = 1\n",
    "    for i in range(n):\n",
    "        row[i] = cur_alpha\n",
    "        cur_alpha *= alpha\n",
    "    return row\n",
    "\n",
    "\n",
    "def vdm(alpha_vec):\n",
    "    n = alpha_vec.size\n",
    "    A = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        A[i, :] = vdm_row(alpha_vec[i], n)\n",
    "    return A\n",
    "\n",
    "# check our implementation against the in-built numpy function\n",
    "x = np.linspace(0, 1, 10)\n",
    "print('Our vdm function agrees with numpy.vander:', np.allclose(vdm(x), np.vander(x, increasing=True)))\n",
    "\n",
    "nns = 10\n",
    "ns = np.linspace(3, 3+nns-1, nns, dtype=int)\n",
    "condV = np.zeros(len(ns))\n",
    "for i, n in enumerate(ns):\n",
    "    x = np.linspace(0, 1, n)\n",
    "    V = vdm(x)\n",
    "    condV[i]=np.linalg.cond(V)\n",
    "    print(n, condV[i])\n",
    "\n",
    "# set up figure\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "ax1.semilogy(ns, condV, 'bo-', label='cond(V)')\n",
    "ax1.set_xlabel('$N$', fontsize=16)\n",
    "ax1.set_ylabel('Condition number', fontsize=16)\n",
    "ax1.set_title('Condition number vs problem size (Vendermonde)', fontsize=16)\n",
    "ax1.legend(loc='best', fontsize=16)\n",
    "ax1.grid(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the Newton matrix\n",
    "\n",
    "def Newton_poly_matrix(x):\n",
    "    \"\"\" Evaluate the Newton matrix appearing in Lagrange interpolation\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    N = np.zeros((n, n))\n",
    "    # first column just 1's\n",
    "    N[:,0] = 1\n",
    "    # other columns - make use of previous columns\n",
    "    for j in range(1,n):\n",
    "        for i in range(j,n):\n",
    "            N[i,j] = (x[i] - x[j-1])*N[i,j-1]\n",
    "    return N\n",
    "\n",
    "nns = 10\n",
    "ns = np.linspace(3,3+nns-1,nns, dtype=int)\n",
    "condN = np.zeros(len(ns))\n",
    "for i, n in enumerate(ns):\n",
    "    x = np.linspace(0, 1, n)\n",
    "    N = Newton_poly_matrix(x)\n",
    "    condN[i]=np.linalg.cond(N)\n",
    "    print(n, condN[i])\n",
    "\n",
    "# set up figure\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "ax1.semilogy(ns, condN, 'bo-', label='cond(V)')\n",
    "ax1.set_xlabel('$N$', fontsize=16)\n",
    "ax1.set_ylabel('Condition number', fontsize=16)\n",
    "ax1.set_title('Condition number vs problem size (Newton)', fontsize=16)\n",
    "ax1.legend(loc='best', fontsize=16)\n",
    "ax1.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework - Hilbert matrix  [$\\star$]\n",
    "\n",
    "NB. this is a repeat of a homework question from L4 of ACSE-2.\n",
    "\n",
    "The *Hilbert matrix* is a classic example of ill-conditioned matrix:\n",
    "\n",
    "$$\n",
    "A = \n",
    "  \\begin{pmatrix}\n",
    "    1      & 1/2    & 1/3    & \\cdots \\\\\n",
    "    1/2    & 1/3    & 1/4    & \\cdots \\\\\n",
    "    1/3    & 1/4    & 1/5    & \\cdots \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots  \\\\\n",
    "\\end{pmatrix}\\,.\n",
    "$$\n",
    "\n",
    "Let's consider the linear system $A\\boldsymbol{x}=\\boldsymbol{b}$ where \n",
    "\n",
    "$$ b_i = \\sum_{j=1}^n a_{ij},\\;\\;\\; \\text{for}\\;\\;\\;\\; i=1,2,\\ldots, n.$$\n",
    "\n",
    "- How can you write entry $A_{ij}$ for any $i$ and $j$ ?\n",
    "\n",
    "\n",
    "- Convince yourself by pen and paper that $ \\boldsymbol{x} = \\left[ 1, 1, \\cdots 1\\right]^T$ is the solution of the system.\n",
    "\n",
    "\n",
    "- Write a function that returns $A$ and $b$ for a given $n$.\n",
    "\n",
    "\n",
    "- For a range of $n$, compute the condition number of $A$, solve the linear system and compute the error ($err = \\sum_{i=1}^n \\left|x_{computed, i}-x_{exact, i}\\right|$). What do you observe ?\n",
    "\n",
    "\n",
    "- Hint: take a look at the 'error_vs_condition_number' figure from the lecutre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution - Hilbert matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hilbert(n):\n",
    "    A = np.zeros((n, n))\n",
    "    b = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            A[i, j] = 1./(i+j+1)\n",
    "        b[i] = np.sum(A[i, :])\n",
    "    return A, b\n",
    "\n",
    "n_max = 14\n",
    "ns = range(2, n_max)\n",
    "conds = np.zeros_like(ns, dtype=float)\n",
    "errors = np.zeros_like(ns, dtype=float)\n",
    "for i, n in enumerate(ns):\n",
    "    A, b = hilbert(n)\n",
    "    x = np.linalg.solve(A, b)\n",
    "    #x = sl.solve(A, b)\n",
    "    x_exact = np.ones(n)\n",
    "    error_1 = np.sum(np.abs(x - x_exact)) / np.sum(np.abs(x_exact))\n",
    "    error_inf = np.max(np.abs(x - x_exact)) / np.max(np.abs(x_exact))\n",
    "    error_fro = np.sqrt(np.sum((x - x_exact)**2)) / np.sqrt(np.sum((x_exact)**2))\n",
    "    errors[i] = error_fro\n",
    "    print()\n",
    "    conds[i] = np.linalg.cond(A, 'fro')\n",
    "    print('{0:2d}, {1:.9e}, {2:.9e}, {3:.9e}, {4:.9e}'.format(n, conds[i], error_1, error_inf, error_fro))\n",
    "\n",
    "print('machine epsilon = ', np.finfo(float).eps)\n",
    "\n",
    "\n",
    "# set up figure\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "ax1.loglog(conds, errors, 'ko-', label='Error')\n",
    "ax1.loglog(conds, conds * np.finfo(float).eps , 'bo-', label='Condition number * eps')\n",
    "ax1.set_xlabel('Condition number', fontsize=16)\n",
    "ax1.set_ylabel('Relative error', fontsize=16)\n",
    "ax1.set_title('Error vs condition number', fontsize=16)\n",
    "ax1.legend(loc='best', fontsize=16)\n",
    "ax1.grid(True)\n",
    "\n",
    "# save figure to use in lecture\n",
    "#fig.savefig('error_vs_condition_number.png', dpi=600, format='png', facecolor='w', edgecolor='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework - Comparison of iterative methods  [$\\star$]\n",
    "\n",
    "Now that we have implemented several iterative solvers, compare their performance (residual vs iteration number) on the following matrix\n",
    "\n",
    "```Python\n",
    "# diagonally dominant matrix (random)\n",
    "n = 1000\n",
    "main_diag = np.ones(n)  \n",
    "off_diag = np.random.random(n-1)\n",
    "A = np.diag(-2*main_diag, 0) + np.diag(1.*off_diag, 1) + np.diag(1.*off_diag, -1)\n",
    "b = np.random.random(A.shape[0])\n",
    "```\n",
    "\n",
    "Hint: the 'direct_vs_iterative' figure from the lecture may provide some motivation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution - Comparison of iterative methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "# CG\n",
    "residuals_cg = []\n",
    "def callback(xk):\n",
    "    residuals_cg.append(np.linalg.norm( ( A @ xk ) - b ) )\n",
    "    \n",
    "# Jacobi\n",
    "def jacobi(A, b, x0, residual_atol = 1.e-10, it_max=500):\n",
    "    x = np.zeros_like(b)\n",
    "    x_new = np.zeros_like(b)\n",
    "    residuals = []\n",
    "    for k in range(it_max):\n",
    "        for i in range(A.shape[0]):\n",
    "            x_new[i] = (1./A[i, i]) * ( b[i] - ( A[i, :i] @ x[:i] ) - ( A[i, i+1:] @ x[i+1:] ) ) \n",
    "        residual = np.linalg.norm( ( A @ x_new ) - b )\n",
    "        residuals.append(residual)  \n",
    "        if (residual < residual_atol):\n",
    "            break\n",
    "        x = np.copy(x_new) \n",
    "    return x, residuals    \n",
    "\n",
    "# G-S if omega = 1\n",
    "residuals_gs = []\n",
    "def gauss_seidel_rel_V1(A, b, omega, maxit=500, tol=1.e-6):\n",
    "    m, n = A.shape\n",
    "    x = np.zeros_like(b)\n",
    "    for k in range(maxit):\n",
    "        for i in range(m):\n",
    "            x[i] = omega/A[i, i] * ( b[i] - ( A[i, :i] @ x[:i] ) - ( A[i, i+1:] @ x[i+1:] ) ) + (1. - omega)*x[i]\n",
    "        residual = np.linalg.norm(( A @ x ) - b)\n",
    "        residuals_gs.append(residual) \n",
    "        if (residual < tol):\n",
    "            break\n",
    "    return x, k\n",
    "\n",
    "# SOR\n",
    "residuals_sor = []\n",
    "def gauss_seidel_rel_V2(A, b, k_relax=10, omega_ini=1., maxit=500, tol=1.e-6):\n",
    "    m, n = A.shape\n",
    "    x = np.zeros_like(b)\n",
    "    omega = omega_ini\n",
    "    for k in range(maxit):\n",
    "        for i in range(m):\n",
    "            x[i] = omega/A[i,i] * ( b[i] - ( A[i, :i] @ x[:i] ) - ( A[i, i+1:] @ x[i+1:] ) ) + (1. - omega)*x[i]\n",
    "        residual = np.linalg.norm(( A @ x ) - b)\n",
    "        residuals_sor.append(residual)  \n",
    "        if (residual < tol): break       \n",
    "        if (k == k_relax): \n",
    "            res_prev = residual\n",
    "        if (k == k_relax+1): \n",
    "            res = residual\n",
    "            omega = 2/(1+sqrt(1-res/res_prev))\n",
    "    return x,k,omega\n",
    "\n",
    "# diagonally dominant matrix (random)\n",
    "n = 1000\n",
    "main_diag = np.ones(n)  \n",
    "off_diag = np.random.random(n-1)\n",
    "A = np.diag(-2*main_diag, 0) + np.diag(1.*off_diag, 1) + np.diag(1.*off_diag, -1)\n",
    "b = np.random.random(A.shape[0])\n",
    "\n",
    "# call solvers\n",
    "x = sl.solve(A,b)\n",
    "x_jac, residuals_jac = jacobi(A, b, np.zeros_like(b), 1.e-10, 1000)\n",
    "print('SciPy and Jacobi solutions agree: ',np.allclose(x, x_jac))\n",
    "x_gs, k = gauss_seidel_rel_V1(A, b, omega=1., maxit=1000, tol=1.e-10)\n",
    "print('SciPy and G-S solutions agree: ',np.allclose(x, x_gs))\n",
    "x_sor, i, omega = gauss_seidel_rel_V2(A, b, omega_ini=1., k_relax=6, maxit=1000, tol=1.e-10)\n",
    "print('SciPy and SOR solutions agree: ',np.allclose(x, x_sor))\n",
    "x_cg, info = spl.cg(A, b, x0=None, tol=1.e-12, maxiter=1000, callback=callback)\n",
    "print('SciPy and CG solutions agree: ',np.allclose(x, x_sor))\n",
    "\n",
    "# set up figure\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.semilogy(residuals_cg, 'k', label='CG')\n",
    "ax1.semilogy(residuals_jac, 'b', label='Jacobi')\n",
    "ax1.semilogy(residuals_gs, 'r', label='G-S')\n",
    "ax1.semilogy(residuals_sor, 'g', label='SOR')\n",
    "ax1.set_xlabel('Iteration', fontsize=16)\n",
    "ax1.set_ylabel('Norm of residual', fontsize=16)\n",
    "ax1.set_title('Solver convergence', fontsize=16)\n",
    "ax1.legend(loc='best')\n",
    "ax1.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test another matrix\n",
    "\n",
    "n = 3000\n",
    "A = 5*np.eye(n)\n",
    "for i in range(n-1):\n",
    "    A[i,i+1] = -2.\n",
    "    A[i+1,i] = -2.\n",
    "b = np.zeros(n)\n",
    "b[n-1] = 1000.\n",
    "\n",
    "residuals_cg = []\n",
    "residuals_gs = []\n",
    "residuals_sor = []\n",
    "# call solvers\n",
    "x = sl.solve(A,b)\n",
    "x_jac, residuals_jac = jacobi(A, b, np.zeros_like(b), 1.e-10, 1000)\n",
    "print('SciPy and Jacobi solutions agree: ',np.allclose(x, x_jac))\n",
    "x_gs, k = gauss_seidel_rel_V1(A, b, omega=1., maxit=1000, tol=1.e-10)\n",
    "print('SciPy and G-S solutions agree: ',np.allclose(x, x_gs))\n",
    "x_sor, i, omega = gauss_seidel_rel_V2(A, b, omega_ini=1., k_relax=6, maxit=1000, tol=1.e-10)\n",
    "print('SciPy and SOR solutions agree: ',np.allclose(x, x_sor))\n",
    "x_cg, info = spl.cg(A, b, x0=None, tol=1.e-12, maxiter=1000, callback=callback)\n",
    "print('SciPy and CG solutions agree: ',np.allclose(x, x_sor))\n",
    "\n",
    "# set up figure\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.semilogy(residuals_cg, 'k', label='CG')\n",
    "ax1.semilogy(residuals_jac, 'b', label='Jacobi')\n",
    "ax1.semilogy(residuals_gs, 'r', label='G-S')\n",
    "ax1.semilogy(residuals_sor, 'g', label='SOR')\n",
    "ax1.set_xlabel('Iteration', fontsize=16)\n",
    "ax1.set_ylabel('Norm of residual', fontsize=16)\n",
    "ax1.set_title('Solver convergence', fontsize=16)\n",
    "ax1.legend(loc='best')\n",
    "ax1.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare some timings \n",
    "\n",
    "# NB. Takes a long time to run the full version:\n",
    "# ns = np.array([1000,2000,4000,8000,16000,32000])\n",
    "ns = np.array([1000,2000,4000,8000])\n",
    "times_cg1 = np.zeros_like(ns,dtype=float)\n",
    "times_cg2 = np.zeros_like(ns,dtype=float)\n",
    "times_ss = np.zeros_like(ns,dtype=float)    \n",
    "\n",
    "# call solvers\n",
    "for i,n in enumerate(ns):\n",
    "    # diagonally dominant matrix (random) - \n",
    "    # note the caveat that we are using dense matrix storage here even though the matrix is sparse.\n",
    "    main_diag = np.ones(n)  \n",
    "    off_diag = np.random.random(n-1)\n",
    "    A = np.diag(2*main_diag, 0) + np.diag(-1.*off_diag, 1) + np.diag(-1.*off_diag, -1)\n",
    "    b = np.random.random(A.shape[0])    \n",
    "    tss = %timeit -n 3 -r 1 -o xs=sl.solve(A,b)\n",
    "    tcg1 = %timeit -n 3 -r 1 -o x_cg, info = spl.cg(A, b, x0=None, tol=1.e-12, maxiter=1000)\n",
    "    tcg2 = %timeit -n 3 -r 1 -o x_cg, info = spl.cg(A, b, x0=None, tol=1.e-8, maxiter=1000)\n",
    "    print(tss.best, tcg1.best, tcg2.best)\n",
    "    times_cg1[i] = tcg1.best\n",
    "    times_cg2[i] = tcg2.best\n",
    "    times_ss[i] = tss.best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up figure\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "ax1.loglog(ns, times_cg1, 'ko-', label='CG. tol=1e-12')\n",
    "ax1.loglog(ns, times_cg2, 'bo-', label='CG. tol=1e-8')\n",
    "ax1.loglog(ns, times_ss, 'ro-', label='SciPy solve')\n",
    "ax1.set_xlabel('$n$', fontsize=16)\n",
    "ax1.set_ylabel('%Timeit best time', fontsize=16)\n",
    "ax1.set_title('Direct vs iterative solver timings', fontsize=16)\n",
    "ax1.legend(loc='best', fontsize=16)\n",
    "ax1.grid(True)\n",
    "\n",
    "# save figure to use in lecture\n",
    "#fig.savefig('direct_vs_iterative.png', dpi=600, format='png', facecolor='w', edgecolor='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework - Cubic splines (from Lecture 1)  [$\\star\\star$]\n",
    "\n",
    "If you didn't attempt it at the time go back and look at the homework exercise from lecture 1 on the generation of a cubic spline approximation to data via the formulation and solve of a linear system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution - Cubic splines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cubic_spline_k(xi, yi):\n",
    "    \"\"\" Compute the second derivative values of the spline at the data location.\n",
    "    \"\"\" \n",
    "    n = len(xi) - 1 # we have n+1 data points \n",
    "    # form the tridiagonal matrix for the k's\n",
    "    del_x = np.diff(xi) # same as xi[1:] - xi[:-1]\n",
    "    del_y = np.diff(yi)\n",
    "    Amat = np.zeros((n+1,n+1))\n",
    "    RHS = np.zeros(n+1)\n",
    "    main_diag = np.zeros(n+1)\n",
    "    lower_diag = np.zeros(n)\n",
    "    upper_diag = np.zeros(n)\n",
    "    main_diag[1:-1]  = 2. * (del_x[:-1] + del_x[1:])\n",
    "    main_diag[0] = 1.\n",
    "    main_diag[-1] = 1.\n",
    "    lower_diag[:-1] = del_x[:-1]\n",
    "    lower_diag[-1] = 0.\n",
    "    upper_diag[1:] = del_x[1:]\n",
    "    upper_diag[0] = 0.\n",
    "    Amat = np.diag(lower_diag, -1) + np.diag(main_diag, 0) + np.diag(upper_diag, 1)  \n",
    "    RHS[1:-1] = 6. * ( del_y[1:]/del_x[1:] - del_y[:-1]/del_x[:-1] )\n",
    "    RHS[0] = 0.\n",
    "    RHS[-1] = 0.\n",
    "    # solve the linear system - see L3 for details\n",
    "    k = np.linalg.solve(Amat, RHS)\n",
    "    return k\n",
    "\n",
    "# This is a little test with solution from Kiusalaas\n",
    "print('Test (Kiusalaas Ex. 3.7): ',np.allclose(cubic_spline_k(np.array([1,2,3,4,5]), np.array([0,1,0,1,0])), \\\n",
    "                           np.array([0.,-30./7.,36./7.,-30./7.,0.])))\n",
    "\n",
    "\n",
    "def evaluate_cubic_spline(xi,yi,xx):\n",
    "    \"\"\" Construct the cubic spline passing through the points (xi,yi)\n",
    "    with natural boundary conditions.\n",
    "    \n",
    "    Return the evaluation of the cubic spline at the locations in xx.\n",
    "    \"\"\" \n",
    "    k = cubic_spline_k(xi,yi)\n",
    "    yy = np.zeros_like(xx)\n",
    "    # we have two options here, we could loop over the intervals and find and evaluate all xx locations \n",
    "    # within it, or instead loop over the xx values and for each find the interval. The former will likely\n",
    "    # be more efficient, but the second perhaps simpler to understand and code - let's do the second!\n",
    "    for n, x in enumerate(xx):\n",
    "        # first find the interval this x lies within\n",
    "        # first two conditionals here allow us to extrapolate\n",
    "        if x < xi[0]:\n",
    "            i = 0\n",
    "        elif x > xi[-1]:\n",
    "            i = len(xi) - 2\n",
    "        else:\n",
    "            i = np.argwhere(x<=xi)[0][0]-1\n",
    "        h = xi[i] - xi[i+1]\n",
    "        # actually evaluate the cubic\n",
    "        yy[n] = ((x - xi[i+1])**3/h - (x - xi[i+1])*h)*k[i]/6.0 \\\n",
    "               - ((x - xi[i])**3/h - (x - xi[i])*h)*k[i+1]/6.0 \\\n",
    "               + (yi[i]*(x - xi[i+1]) - yi[i+1]*(x - xi[i]))/h\n",
    "    return yy\n",
    "\n",
    "# Now see if this recreates the figures obtained using SciPy\n",
    "xi = np.array([0.5, 2.0, 4.0, 5.0, 7.0, 9.0])\n",
    "yi = np.array([0.5, 0.4, 0.3, 0.1, 0.9, 0.8])\n",
    "\n",
    "# set up figure\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.margins(0.1)\n",
    "\n",
    "x = np.linspace(0.2, 9.3, 100)\n",
    "\n",
    "# Plot our cubic spline\n",
    "ax1.plot(x, evaluate_cubic_spline(xi,yi,x), 'b', label='Cubic Spline')\n",
    "\n",
    "def plot_raw_data(xi, yi, ax):\n",
    "    \"\"\"plot x vs y on axes ax, \n",
    "    add axes labels and turn on grid\n",
    "    \"\"\"\n",
    "    ax.plot(xi, yi, 'ko', label='raw data')\n",
    "    ax.set_xlabel('$x$', fontsize=16)\n",
    "    ax.set_ylabel('$y$', fontsize=16)\n",
    "    ax.grid(True)\n",
    "    \n",
    "# Overlay raw data\n",
    "plot_raw_data(xi, yi, ax1)\n",
    "ax1.legend(loc='best', fontsize=14)\n",
    "ax1.set_title('Cubic spline approximation to our data', fontsize=16);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
